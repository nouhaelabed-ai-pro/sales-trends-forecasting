{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('data/Superstore.csv')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'])\n",
    "\n",
    "# Extract date features\n",
    "df['Year'] = df['Order Date'].dt.year\n",
    "df['Month'] = df['Order Date'].dt.month\n",
    "df['Day'] = df['Order Date'].dt.day\n",
    "df['DayOfWeek'] = df['Order Date'].dt.dayofweek\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Row ID', 'Order ID', 'Ship Date', 'Customer Name', \n",
    "                   'Product ID', 'Product Name', 'Country', 'Order Date',\n",
    "                   'Postal Code', 'Customer ID']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Convert categorical variables to numeric using label encoding\n",
    "categorical_columns = ['Ship Mode', 'Segment', 'Category', 'Sub-Category', 'Region', 'State']\n",
    "for col in categorical_columns:\n",
    "    df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "print(\"Data shape after initial preprocessing:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def remove_outliers(df, columns, n_std=3):\n",
    "    \"\"\"\n",
    "    Remove outliers from specified columns using z-score method\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        # Calculate z-scores\n",
    "        z_scores = stats.zscore(df_clean[column])\n",
    "        # Create a mask for rows to keep\n",
    "        mask = np.abs(z_scores) < n_std\n",
    "        df_clean = df_clean[mask]\n",
    "        print(f\"Removed {len(df) - len(df_clean)} outliers from {column}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Columns to check for outliers\n",
    "numeric_columns = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "\n",
    "# Remove outliers\n",
    "df_cleaned = remove_outliers(df, numeric_columns, n_std=3)\n",
    "\n",
    "print(\"\\nOriginal dataset shape:\", df.shape)\n",
    "print(\"Cleaned dataset shape:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and target\n",
    "X = df_cleaned.drop('Sales', axis=1)\n",
    "y = df_cleaned['Sales']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Validation set shape:\", X_valid_scaled.shape)\n",
    "print(\"Test set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and configure the XGBoost model with better parameters\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1000,          # Increase number of trees\n",
    "    learning_rate=0.01,         # Reduce learning rate\n",
    "    max_depth=4,                # Reduce max_depth to prevent overfitting\n",
    "    min_child_weight=5,         # Add min_child_weight to reduce overfitting\n",
    "    subsample=0.8,              # Use only 80% of data per tree\n",
    "    colsample_bytree=0.8,       # Use only 80% of features per tree\n",
    "    reg_alpha=0.1,              # L1 regularization\n",
    "    reg_lambda=1.0,             # L2 regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    eval_set=[(X_train_scaled, y_train), (X_valid_scaled, y_valid)],\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,    # Stop if no improvement for 50 rounds\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate RMSE on test set\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "print(f\"\\nTest Set RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Calculate R-squared score\n",
    "r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - y_test.mean()) ** 2)\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
